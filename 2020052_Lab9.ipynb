{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cHmSU08_4W7",
        "outputId": "c6625e0d-3d70-4933-c480-44239ffcac89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.13.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.1.0)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.8 virtualenv-20.25.0\n"
          ]
        }
      ],
      "source": [
        "!pip install virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv /content/cuda_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EldNTA1i_7AT",
        "outputId": "61f39929-bf5f-498b-9bf8-886974036846"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created virtual environment CPython3.10.12.final.0-64 in 773ms\n",
            "  creator CPython3Posix(dest=/content/cuda_env, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==23.3.1, setuptools==69.0.2, wheel==0.42.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source /content/cuda_env/bin/activate;"
      ],
      "metadata": {
        "id": "TlsFnJwsAiM7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBoXdLD6AoeI",
        "outputId": "619110f2-f14b-4f96-8e27-48a6c5e3d618"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-9wm7nqsq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-9wm7nqsq\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4293 sha256=be4d9e2bb27c7673f87e4f32bb1221f268450edeb529285baf6f0c3f1f8cfcd6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rbmb2bvz/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TEST CASE 1**"
      ],
      "metadata": {
        "id": "6FOY5BolBBHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "const int N = 3;\n",
        "\n",
        "// Kernel using shared memory\n",
        "__global__ void matrixMulShared(const int *A, const int *B, int *C, int n) {\n",
        "    __shared__ int tileA[N][N];\n",
        "    __shared__ int tileB[N][N];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int result = 0;\n",
        "\n",
        "    // Loop over tiles\n",
        "    for (int t = 0; t < n; t += blockDim.x) {\n",
        "        // Load tiles into shared memory\n",
        "        tileA[threadIdx.y][threadIdx.x] = (row < n && t + threadIdx.x < n) ? A[row * n + t + threadIdx.x] : 0;\n",
        "        tileB[threadIdx.y][threadIdx.x] = (col < n && t + threadIdx.y < n) ? B[(t + threadIdx.y) * n + col] : 0;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute partial result from the tiles in shared memory\n",
        "        for (int k = 0; k < blockDim.x; ++k) {\n",
        "            result += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result to the output matrix\n",
        "    if (row < n && col < n) {\n",
        "        C[row * n + col] = result;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel using global memory\n",
        "__global__ void matrixMulGlobal(const int *A, const int *B, int *C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int result = 0;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        for (int k = 0; k < n; ++k) {\n",
        "            result += A[row * n + k] * B[k * n + col];\n",
        "        }\n",
        "        C[row * n + col] = result;\n",
        "    }\n",
        "}\n",
        "\n",
        "// CPU implementation\n",
        "void matrixMulCPU(const int *A, const int *B, int *C, int n) {\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            int result = 0;\n",
        "            for (int k = 0; k < n; ++k) {\n",
        "                result += A[i * n + k] * B[k * n + j];\n",
        "            }\n",
        "            C[i * n + j] = result;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Utility function to print matrices\n",
        "void printMatrix(const int *matrix, int rows, int cols) {\n",
        "    for (int i = 0; i < rows; ++i) {\n",
        "        for (int j = 0; j < cols; ++j) {\n",
        "            std::cout << std::setw(5) << matrix[i * cols + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int A[N][N] = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};\n",
        "    const int B[N][N] = {{9, 8, 7}, {6, 5, 4}, {3, 2, 1}};\n",
        "\n",
        "    int C_cpu[N][N];\n",
        "    int C_gpu_shared[N][N];\n",
        "    int C_gpu_global[N][N];\n",
        "\n",
        "    int *d_A, *d_B, *d_C_shared, *d_C_global;\n",
        "    cudaMalloc((void **)&d_A, N * N * sizeof(int));\n",
        "    cudaMalloc((void **)&d_B, N * N * sizeof(int));\n",
        "    cudaMalloc((void **)&d_C_shared, N * N * sizeof(int));\n",
        "    cudaMalloc((void **)&d_C_global, N * N * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_A, &A[0][0], N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, &B[0][0], N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockDim(N, N);\n",
        "    dim3 gridDim(1, 1);\n",
        "\n",
        "    // Measure GPU execution time for shared memory\n",
        "    auto start_gpu_shared = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulShared<<<gridDim, blockDim>>>(d_A, d_B, d_C_shared, N);\n",
        "    cudaDeviceSynchronize(); // Wait for GPU to finish\n",
        "    auto end_gpu_shared = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> gpu_shared_duration = end_gpu_shared - start_gpu_shared;\n",
        "    std::cout << \"GPU (Shared Memory) Execution Time: \" << gpu_shared_duration.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Measure GPU execution time for global memory\n",
        "    auto start_gpu_global = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulGlobal<<<gridDim, blockDim>>>(d_A, d_B, d_C_global, N);\n",
        "    cudaDeviceSynchronize(); // Wait for GPU to finish\n",
        "    auto end_gpu_global = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> gpu_global_duration = end_gpu_global - start_gpu_global;\n",
        "    std::cout << \"GPU (Global Memory) Execution Time: \" << gpu_global_duration.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Copy results from GPU to host\n",
        "    cudaMemcpy(&C_gpu_shared[0][0], d_C_shared, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&C_gpu_global[0][0], d_C_global, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Measure CPU execution time\n",
        "    auto start_cpu = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulCPU(&A[0][0], &B[0][0], &C_cpu[0][0], N);\n",
        "    auto end_cpu = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> cpu_duration = end_cpu - start_cpu;\n",
        "    std::cout << \"CPU Execution Time: \" << cpu_duration.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Print matrices A, B, and C_cpu (result from CPU)\n",
        "    std::cout << \"Matrix A:\" << std::endl;\n",
        "    printMatrix(&A[0][0], N, N);\n",
        "\n",
        "    std::cout << \"Matrix B:\" << std::endl;\n",
        "    printMatrix(&B[0][0], N, N);\n",
        "\n",
        "    std::cout << \"Result from CPU (C_cpu):\" << std::endl;\n",
        "    printMatrix(&C_cpu[0][0], N, N);\n",
        "\n",
        "    // Print the result from the GPU (C_gpu_shared)\n",
        "    std::cout << \"Result from GPU (Shared Memory) (C_gpu_shared):\" << std::endl;\n",
        "    printMatrix(&C_gpu_shared[0][0], N, N);\n",
        "\n",
        "    // Print the result from the GPU (C_gpu_global)\n",
        "    std::cout << \"Result from GPU (Global Memory) (C_gpu_global):\" << std::endl;\n",
        "    printMatrix(&C_gpu_global[0][0], N, N);\n",
        "\n",
        "    // Compare results\n",
        "    bool resultMatch_shared = true;\n",
        "    bool resultMatch_global = true;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            if (C_cpu[i][j] != C_gpu_shared[i][j]) {\n",
        "                resultMatch_shared = false;\n",
        "            }\n",
        "            if (C_cpu[i][j] != C_gpu_global[i][j]) {\n",
        "                resultMatch_global = false;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (resultMatch_shared) {\n",
        "        std::cout << \"Results match between CPU and GPU (Shared Memory) implementations.\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Results do not match between CPU and GPU (Shared Memory) implementations.\" << std::endl;\n",
        "    }\n",
        "\n",
        "    if (resultMatch_global) {\n",
        "        std::cout << \"Results match between CPU and GPU (Global Memory) implementations.\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Results do not match between CPU and GPU (Global Memory) implementations.\" << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C_shared);\n",
        "    cudaFree(d_C_global);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UzsEtiTAsE8",
        "outputId": "c3df9fad-91e2-43f3-806e-8515e69edc8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU (Shared Memory) Execution Time: 0.127965 seconds\n",
            "GPU (Global Memory) Execution Time: 4.1126e-05 seconds\n",
            "CPU Execution Time: 5.98e-07 seconds\n",
            "Matrix A:\n",
            "    1     2     3 \n",
            "    4     5     6 \n",
            "    7     8     9 \n",
            "\n",
            "Matrix B:\n",
            "    9     8     7 \n",
            "    6     5     4 \n",
            "    3     2     1 \n",
            "\n",
            "Result from CPU (C_cpu):\n",
            "   30    24    18 \n",
            "   84    69    54 \n",
            "  138   114    90 \n",
            "\n",
            "Result from GPU (Shared Memory) (C_gpu_shared):\n",
            "   30    24    18 \n",
            "   84    69    54 \n",
            "  138   114    90 \n",
            "\n",
            "Result from GPU (Global Memory) (C_gpu_global):\n",
            "   30    24    18 \n",
            "   84    69    54 \n",
            "  138   114    90 \n",
            "\n",
            "Results match between CPU and GPU (Shared Memory) implementations.\n",
            "Results match between CPU and GPU (Global Memory) implementations.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST CASE 2**"
      ],
      "metadata": {
        "id": "7rjkpupNBPmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "const int N = 3;\n",
        "\n",
        "// Kernel using shared memory\n",
        "__global__ void matrixMulShared(const int *A, const int *B, int *C, int n) {\n",
        "    __shared__ int tileA[N][N];\n",
        "    __shared__ int tileB[N][N];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int result = 0;\n",
        "\n",
        "    // Loop over tiles\n",
        "    for (int t = 0; t < n; t += blockDim.x) {\n",
        "        // Load tiles into shared memory\n",
        "        tileA[threadIdx.y][threadIdx.x] = (row < n && t + threadIdx.x < n) ? A[row * n + t + threadIdx.x] : 0;\n",
        "        tileB[threadIdx.y][threadIdx.x] = (col < n && t + threadIdx.y < n) ? B[(t + threadIdx.y) * n + col] : 0;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute partial result from the tiles in shared memory\n",
        "        for (int k = 0; k < blockDim.x; ++k) {\n",
        "            result += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result to the output matrix\n",
        "    if (row < n && col < n) {\n",
        "        C[row * n + col] = result;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel using global memory\n",
        "__global__ void matrixMulGlobal(const int *A, const int *B, int *C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int result = 0;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        for (int k = 0; k < n; ++k) {\n",
        "            result += A[row * n + k] * B[k * n + col];\n",
        "        }\n",
        "        C[row * n + col] = result;\n",
        "    }\n",
        "}\n",
        "\n",
        "// CPU implementation\n",
        "void matrixMulCPU(const int *A, const int *B, int *C, int n) {\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            int result = 0;\n",
        "            for (int k = 0; k < n; ++k) {\n",
        "                result += A[i * n + k] * B[k * n + j];\n",
        "            }\n",
        "            C[i * n + j] = result;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Utility function to print matrices\n",
        "void printMatrix(const int *matrix, int rows, int cols) {\n",
        "    for (int i = 0; i < rows; ++i) {\n",
        "        for (int j = 0; j < cols; ++j) {\n",
        "            std::cout << std::setw(5) << matrix[i * cols + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int A[N][N] = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};\n",
        "    const int B[N][N] = {{9, 15, 12}, {52, -52, 15}, {-21, 52, 21}};\n",
        "\n",
        "    int C_cpu[N][N];\n",
        "    int C_gpu_shared[N][N];\n",
        "    int C_gpu_global[N][N];\n",
        "\n",
        "    int *d_A, *d_B, *d_C_shared, *d_C_global;\n",
        "    cudaMalloc((void **)&d_A, N * N * sizeof(int));\n",
        "    cudaMalloc((void **)&d_B, N * N * sizeof(int));\n",
        "    cudaMalloc((void **)&d_C_shared, N * N * sizeof(int));\n",
        "    cudaMalloc((void **)&d_C_global, N * N * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_A, &A[0][0], N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, &B[0][0], N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockDim(N, N);\n",
        "    dim3 gridDim(1, 1);\n",
        "\n",
        "    // Measure GPU execution time for shared memory\n",
        "    auto start_gpu_shared = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulShared<<<gridDim, blockDim>>>(d_A, d_B, d_C_shared, N);\n",
        "    cudaDeviceSynchronize(); // Wait for GPU to finish\n",
        "    auto end_gpu_shared = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> gpu_shared_duration = end_gpu_shared - start_gpu_shared;\n",
        "    std::cout << \"GPU (Shared Memory) Execution Time: \" << gpu_shared_duration.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Measure GPU execution time for global memory\n",
        "    auto start_gpu_global = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulGlobal<<<gridDim, blockDim>>>(d_A, d_B, d_C_global, N);\n",
        "    cudaDeviceSynchronize(); // Wait for GPU to finish\n",
        "    auto end_gpu_global = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> gpu_global_duration = end_gpu_global - start_gpu_global;\n",
        "    std::cout << \"GPU (Global Memory) Execution Time: \" << gpu_global_duration.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Copy results from GPU to host\n",
        "    cudaMemcpy(&C_gpu_shared[0][0], d_C_shared, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&C_gpu_global[0][0], d_C_global, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Measure CPU execution time\n",
        "    auto start_cpu = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulCPU(&A[0][0], &B[0][0], &C_cpu[0][0], N);\n",
        "    auto end_cpu = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> cpu_duration = end_cpu - start_cpu;\n",
        "    std::cout << \"CPU Execution Time: \" << cpu_duration.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Print matrices A, B, and C_cpu (result from CPU)\n",
        "    std::cout << \"Matrix A:\" << std::endl;\n",
        "    printMatrix(&A[0][0], N, N);\n",
        "\n",
        "    std::cout << \"Matrix B:\" << std::endl;\n",
        "    printMatrix(&B[0][0], N, N);\n",
        "\n",
        "    std::cout << \"Result from CPU (C_cpu):\" << std::endl;\n",
        "    printMatrix(&C_cpu[0][0], N, N);\n",
        "\n",
        "    // Print the result from the GPU (C_gpu_shared)\n",
        "    std::cout << \"Result from GPU (Shared Memory) (C_gpu_shared):\" << std::endl;\n",
        "    printMatrix(&C_gpu_shared[0][0], N, N);\n",
        "\n",
        "    // Print the result from the GPU (C_gpu_global)\n",
        "    std::cout << \"Result from GPU (Global Memory) (C_gpu_global):\" << std::endl;\n",
        "    printMatrix(&C_gpu_global[0][0], N, N);\n",
        "\n",
        "    // Compare results\n",
        "    bool resultMatch_shared = true;\n",
        "    bool resultMatch_global = true;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            if (C_cpu[i][j] != C_gpu_shared[i][j]) {\n",
        "                resultMatch_shared = false;\n",
        "            }\n",
        "            if (C_cpu[i][j] != C_gpu_global[i][j]) {\n",
        "                resultMatch_global = false;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (resultMatch_shared) {\n",
        "        std::cout << \"Results match between CPU and GPU (Shared Memory) implementations.\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Results do not match between CPU and GPU (Shared Memory) implementations.\" << std::endl;\n",
        "    }\n",
        "\n",
        "    if (resultMatch_global) {\n",
        "        std::cout << \"Results match between CPU and GPU (Global Memory) implementations.\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Results do not match between CPU and GPU (Global Memory) implementations.\" << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C_shared);\n",
        "    cudaFree(d_C_global);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y2vUa0iA4YY",
        "outputId": "8e95b202-a9ca-4aa2-90f3-379efa2fbcef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU (Shared Memory) Execution Time: 0.000233113 seconds\n",
            "GPU (Global Memory) Execution Time: 3.6068e-05 seconds\n",
            "CPU Execution Time: 5.52e-07 seconds\n",
            "Matrix A:\n",
            "    1     2     3 \n",
            "    4     5     6 \n",
            "    7     8     9 \n",
            "\n",
            "Matrix B:\n",
            "    9    15    12 \n",
            "   52   -52    15 \n",
            "  -21    52    21 \n",
            "\n",
            "Result from CPU (C_cpu):\n",
            "   50    67   105 \n",
            "  170   112   249 \n",
            "  290   157   393 \n",
            "\n",
            "Result from GPU (Shared Memory) (C_gpu_shared):\n",
            "   50    67   105 \n",
            "  170   112   249 \n",
            "  290   157   393 \n",
            "\n",
            "Result from GPU (Global Memory) (C_gpu_global):\n",
            "   50    67   105 \n",
            "  170   112   249 \n",
            "  290   157   393 \n",
            "\n",
            "Results match between CPU and GPU (Shared Memory) implementations.\n",
            "Results match between CPU and GPU (Global Memory) implementations.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCwpUwL2BxrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}